zz <- file(myFileName, "w")
write.csv(myfile, zz, row.names=FALSE)
close(zz)
}
#And add it to the giant dataframe
if (nrow(all_mean_signal) == 0){
all_mean_signal = myfile
}else{
all_mean_signal = rbind(all_mean_signal, myfile)
}
}
#Now do the PL2017 ones...
myResultsPath
mycsvfile = read.csv(paste(myResultsPath, 'ToMfROIS_resp_Cloudy_PL2017_20180312', '.csv', sep=""))
mycsvfile
mycsvfile$filename = result
result = str_replace(result, 'mentpain_','')#Small fix to remove ment-pain string from Cloudy filenames
mydetails = str_split_fixed(result, '_', 4)
mycsvfile$fROIs <- mydetails[[1]]
mycsvfile$task <- mydetails[[3]]
mycsvfile$ind_selection_method <- 'Top10Percent' #All use standard voxel selection
mycsvfile$pipeline = 'PL2017'
unique(mycsvfile$Contrast)
names(mycsvfile)
names(all_mean_signal
)
#Now do the PL2017 ones...
for (result in PL2017Results){
#Get the csv produced by convert_spmss_results.
setwd(myResultsPath)
mycsvfile = read.csv(paste(myResultsPath, result, '.csv', sep=""))
mycsvfile$filename = result
result = str_replace(result, 'mentpain_','')#Small fix to remove ment-pain string from Cloudy filenames
mydetails = str_split_fixed(result, '_', 4)
mycsvfile$fROIs <- mydetails[[1]]
mycsvfile$task <- mydetails[[3]]
mycsvfile$ind_selection_method <- 'Top10Percent' #All use standard voxel selection
mycsvfile$pipeline = 'PL2017'
#Optional: print back out a nice file (replaces convert_spmss_results, with added extra columns)
if(toSave){
setwd(myOutputPath)
myFileName = paste(result,'.csv', sep="")
zz <- file(myFileName, "w")
write.csv(myfile, zz, row.names=FALSE)
close(zz)
}
#And add it to the giant dataframe
if (nrow(all_mean_signal) == 0){
all_mean_signal = mycsvfile
}else{
all_mean_signal = rbind(all_mean_signal, mycsvfile)
}
}
View(filter(all_mean_signal, task == 'Cloudy'))
getwd()
setwd(myOutputPath)
write.csv(all_mean_signal, 'all_mean_signal_outputs.csv', row.names = FALSE)
# Start here to reproduce the main (confirmatory) analyses for the Jokes paper
#This file reads in ALL the %-signal-change values, per-participant, per-parcel, per-contrast,
# Those %-signal-change calculations are produced by the awesome toolbox analyses, and represent a single overall calculation
#derived for the whole parcel region (not individual voxels, as mk sometimes forgets)
#ALL packages necessary for the analysis pipeline should get loaded here
rm(list = ls())
library(bootstrap)
library(dplyr)
library(ggplot2)
library(lme4)
library(pwr)
library(stringr)
library(tidyr)
#Set your working directory here
repodir = "/Users/mekline/Dropbox/_Projects/Jokes - fMRI/Jokes-Replication-Analysis/"
analysis_folder = paste(repodir, 'analysis_pipeline/', sep='')
meansig_outputs_folder = paste(repodir, 'E2_meansignal_outputs/', sep='')
setwd(analysis_folder)
########
#READ IN DATA - OR SKIP TO FILE XXXXXX TO PROCEED FROM CLEANED AND LABELED DATA
########
allSigChange = read.csv(paste(meansig_outputs_folder, 'all_mean_signal_outputs.csv', sep=''))
#FOR NOW: Make a choice whether to do all analyses with top 50 voxels or top 10% voxels
#(This can be changed to 'Top50Voxels' to see all results with that fROI selection procedure
allSigChange = filter(allSigChange, ind_selection_method == 'Top10Percent')
#NEW: Also, screen out the split-half analyses for now; we'll treat those in their own file since they are exploratory
allSigChange = filter(allSigChange, !filename %in% c('SplitHalf_RHLfROIs_resp_Jokes_20170904',
'SplitHalf_LangfROIs_resp_Jokes_20170904',
'SplitHalf_MDfROIs_resp_Jokes_20170904',
'SplitHalf_ToMfROIs_resp_Jokes_20170904'))
#CHECKSUM - The number of rows in allSigChange should be the same
#after the below naming procedure (see line 136)
nrow(allSigChange) #8489
# List contrast and ROI names so it's not just numbers!!!!! (This ordering comes from the
# standard ordering produced by the 2nd level analyses; we'll arrange differently in the plots)
RHLangROI.Names = c('RPost Temp', 'RAnt Temp', 'RAngG', 'RIFG',      'RMFG',     'RIFG orb');
LangROI.Names = c('LPost Temp', 'LAnt Temp', 'LAngG', 'LIFG',      'LMFG',     'LIFG orb');
MDROI.Names = c('LIFG op',  'RIFG op', 'LMFG',    'RMFG',    'LMFG orb',
'RMFG orb', 'LPrecG', 'RPrecG',  'LInsula', 'RInsula',
'LSMA',    'RSMA',   'LPar Inf', 'RPar Inf', 'LPar Sup',
'RPar Sup', 'LACC',   'RACC');
ToMROI.Names = c('DM PFC', 'LTPJ',  'MM PFC', 'PC',
'RTPJ',  'VM PFC', 'RSTS');
joke.contrasts = c('joke', 'lit', 'joke-lit')
jokecustom.contrasts = c('low','med','high', 'linear') #Bug solved! I didn't record 'other' (no response) in the toolbox output this time. NBD.
jokecustom.forcloudy.contrasts = c('low','med','high','other','paramfun') #Apparently used different cond names for these ones.
lang.contrasts = c('S','N','S-N')
MD.contrasts = c('H','E','H-E')
ToM.contrasts = c('bel','pho','bel-pho')
Cloudy.contrasts = c('ment','pain','phys','reln')
#Small fix to make the splitting work right - standardize capitalization!
allSigChange <- allSigChange %>%
mutate(correctedFROIs = as.factor(str_c(str_sub(fROIs, 1, -2), 's')))%>%
mutate(fROIs = correctedFROIs)%>% #(dropping the old fROIs column)
select(-correctedFROIs)
#Split the data into groups by fROI sets, and rename them as appropriate (messy, but done separately to deal with various
#inconsistencies in the data organization)
#(Within each, we check which task applies, and add the crit-contrasts that way)
RHLang_sigs = data.frame(NULL)
LHLang_sigs = data.frame(NULL)
MD_sigs = data.frame(NULL)
ToM_sigs = data.frame(NULL)
CloudyToM_sigs = data.frame(NULL)
RHLang_sigs = allSigChange %>%
filter(fROIs == 'RHLfROIs')%>%
mutate(ROIName = RHLangROI.Names[ROI]) %>%
group_by(task)%>%
mutate(contrastName = ifelse(task == 'Jokes', joke.contrasts[Contrast],
ifelse(task == 'JokesCustom', jokecustom.contrasts[Contrast],
lang.contrasts[Contrast]))) %>%
mutate(ROIMask = 'RHLang') %>%
mutate(localizer = 'Lang') %>%
ungroup()
LHLang_sigs = allSigChange %>%
filter(fROIs == 'LangfROIs')%>%
mutate(ROIName = LangROI.Names[ROI]) %>%
group_by(task)%>%
mutate(contrastName = ifelse(task == 'Jokes', joke.contrasts[Contrast],
ifelse(task == 'JokesCustom', jokecustom.contrasts[Contrast],
lang.contrasts[Contrast]))) %>%
mutate(ROIMask = 'LHLang') %>%
mutate(localizer = 'Lang') %>%
ungroup()
MD_sigs = allSigChange %>%
filter(fROIs == 'MDfROIs')%>%
mutate(ROIName = MDROI.Names[ROI]) %>%
group_by(task)%>%
mutate(contrastName = ifelse(task == 'Jokes', joke.contrasts[Contrast],
ifelse(task == 'JokesCustom', jokecustom.contrasts[Contrast],
MD.contrasts[Contrast]))) %>%
mutate(ROIMask = ifelse(ROI %%2 == 1, 'MDLeft','MDRight')) %>%
mutate(localizer = 'MD') %>%
ungroup()
ToM_sigs = allSigChange %>%
filter(fROIs == 'ToMfROIs')%>%
mutate(ROIName = ToMROI.Names[ROI]) %>%
group_by(task)%>%
mutate(contrastName = ifelse(task == 'Jokes', joke.contrasts[Contrast],
ifelse(task == 'JokesCustom', jokecustom.contrasts[Contrast],
ifelse(task == 'Cloudy', Cloudy.contrasts[Contrast],
ToM.contrasts[Contrast])))) %>%
mutate(ROIMask = 'ToM') %>%
mutate(localizer = 'ToM') %>%
ungroup()
CloudyToM_sigs = allSigChange %>%
filter(fROIs == 'CloudyToMfROIs')%>%
mutate(ROIName = ToMROI.Names[ROI]) %>%
group_by(task) %>%
mutate(contrastName = ifelse(task == 'Jokes', joke.contrasts[Contrast],
ifelse(task == 'JokesCustom', jokecustom.forcloudy.contrasts[Contrast],
Cloudy.contrasts[Contrast]))) %>%
mutate(ROIMask = 'ToM') %>%
mutate(localizer = 'Cloudy') %>%
ungroup()
#And stick it all back together!!
allSigChange = rbind(RHLang_sigs, LHLang_sigs, MD_sigs, ToM_sigs, CloudyToM_sigs)
#CHECKSUM - make sure no analyses were lost and that name matching went correctly!
nrow(allSigChange) #Should be 8489
aggregate(allSigChange$average.localizer.mask.size, by=list(allSigChange$ROIName, allSigChange$ROI.size), mean) #Checks that ROINames were applied correctly!
#Arrange and factorize columns
#Pick allSigChange columns to retain (removing columns with arbitrary numbering systems. Names are better!)
allSigChange <- allSigChange %>%
select(-one_of("ROI", "ROI.size","average.localizer.mask.size","inter.subject.overlap","Contrast", "fROIs"))
allSigChange$SubjectNumber <- as.factor(allSigChange$SubjectNumber)
allSigChange$task <- as.factor(allSigChange$task)
allSigChange$localizer <- as.factor(allSigChange$localizer)
allSigChange$ROIMask <- as.factor(allSigChange$ROIMask)
allSigChange$ROIName <- as.factor(allSigChange$ROIName)
allSigChange$contrastName <- as.factor(allSigChange$contrastName)
#Add average signal change by ROIMask
#In addition to the by-region signal changes, we are going to give each person an average signal change value for each localizer, each task
avgSigChange = aggregate(allSigChange$sigChange, by=list(allSigChange$ROIMask, allSigChange$localizer, allSigChange$task, allSigChange$SubjectNumber,allSigChange$contrastName), mean)
names(avgSigChange) = c('ROIMask', 'localizer', 'task', 'SubjectNumber', 'contrastName','sigChange')
avgSigChange <- avgSigChange %>%
mutate(ROIName = 'LocalizerAverage')
avgSigChange <- allSigChange %>%
group_by(ROIMask, localizer, task, contrastName, SubjectNumber)%>%
summarize(sigChange = mean(sigChange),
filename = first(filename),
ind_selection_method=first(ind_selection_method),
pipeline = first(pipeline),
ROIName = 'LocalizerAverage')
allSigChange <- bind_rows(allSigChange, avgSigChange)
#New plan 1/18/18 - save this df to avoid messy rerunning.
setwd(meansig_outputs_folder)
save(allSigChange, file = "allSigChange.RData")
setwd(analysis_folder)
save(allSigChange, file = "allSigChange.RData")
### DONE 1_load_and_code_jokes.R
rm(list = ls())
library(bootstrap)
library(dplyr)
library(ggplot2)
library(lme4)
library(pwr)
library(stringr)
library(tidyr)
setwd("/Users/mekline/Dropbox/_Projects/Jokes - fMRI/Jokes-Replication-Analysis/analysis_pipeline")
mywd = getwd()
#Make sure allSigChange is loaded. If it's not, load it
load("allSigChange.RData")
View(filter(allSigChange, task=='Cloudy'))
#######
# Calculate T Tests
#######
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
mutate(p.adj = p.adjust(p, method="fdr", n=familySize)) %>%
ungroup()%>%
group_by(ROIMask, localizer, task)%>%
mutate(p.adj = p.adjust(p, method="fdr", n=familySize))
View(allTests)
zz = file('localizer_t_tests_all.csv', 'w')
write.csv(allTests, zz, row.names=FALSE)
close(zz)
########
# Report those T tests like we want for the paper (main text)
########
#Do corrections ever matter?
allTests <- allTests %>%
mutate(sig = p < 0.05) %>%
mutate(sigCor = p.adj < 0.05) %>%
mutate(mismatch = sig != sigCor)
View(filter(allTests,mismatch))
#In the replication set, one mismatch: Nonwords over fixation in the linguistic task, LIFG orb, is
#significant before but not after correction. We don't care about this bc the interesting thing from
#that task is Sentences - Nonwords, not activation over fixation.
#Convention: when all tests go one way, report them together as follows:
reportTests <- function(ts, ps){
if (all(ps > 0.05)){
paste('all insig, ts <', max(ts), 'ps>', min(ps))
} else if (all(ps < 0.05)){
paste('all sig, ts >', min(ts), 'ps<', max(ps))
} else {
'explore...'
}
}
###
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value)
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, localizer, task)%>%
mutate(p.adj = p.adjust(p, method="fdr", n=familySize))
allTests$p.adj
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()
help("p.adjust")
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, localizer, task)%>%
mutate(howmany = length(p))
allTests
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, ROIName, localizer, task)%>%
mutate(p.adj = p.adjust(p, method="fdr", n=familySize))
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, ROIName, localizer, task)%>%
mutate(p.adj = p.adjust(p, method="fdr"))
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, ROIName, localizer, task)%>%
mutate(p.adj = p.adjust(p, method="fdr"), howmany = length(p))
View(allTests)
allTests <- allSigChange %>%
group_by(ROIMask, localizer, task)%>%
summarize(familySize = length(unique(ROIName))) %>%
merge(allSigChange) %>%
group_by(ROIMask, localizer, task, ROIName, contrastName, familySize) %>%
summarise(t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value) %>%
ungroup()%>%
group_by(ROIMask, localizer, task, contrastName)%>%
mutate(p.adj = p.adjust(p, method="fdr"), howmany = length(p))
View(allTests)
View(allTests)
zz = file('localizer_t_tests_all.csv', 'w')
write.csv(allTests, zz, row.names=FALSE)
close(zz)
View(allSigChange)
#Get Jokes values
JokeSigs = allSigChange %>%
filter(contrastName %in% c('joke','lit','high','med','low'))
#Get Jokes values
JokeSigs = allSigChange %>%
filter(contrastName %in% c('joke','lit','high','med','low'))
#Next, get the table that we'll be making the graphs from: for each region (including the average region), take all
#the individual signal changes and calculate a mean and a standard error
sterr <- function(mylist){
my_se = sd(mylist)/sqrt(length(mylist))
return(my_se)
}
mystats = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), mean)
names(mystats) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'themean')
myster = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), sterr)
names(myster) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'sterr')
mystats = merge(mystats,myster)
mystats$se_up = mystats$themean + mystats$sterr
mystats$se_down = mystats$themean - mystats$sterr
mybootup = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootup)
names(mybootup) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootup')
mybootdown = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootdown)
names(mybootdown) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootdown')
mystats = merge(mystats,mybootup)
mystats = merge(mystats,mybootdown)
#Get Jokes values
JokeSigs = allSigChange %>%
filter(contrastName %in% c('joke','lit','high','med','low'))
#Next, get the table that we'll be making the graphs from: for each region (including the average region), take all
#the individual signal changes and calculate a mean and a standard error
sterr <- function(mylist){
my_se = sd(mylist)/sqrt(length(mylist))
return(my_se)
}
mystats = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), mean)
names(mystats) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'themean')
myster = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), sterr)
names(myster) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'sterr')
mystats = merge(mystats,myster)
mystats$se_up = mystats$themean + mystats$sterr
mystats$se_down = mystats$themean - mystats$sterr
#Edit! We should be doing bootstrapped 95% confidence intervals instead! calculate them from allSigChange
#then merge into mystats
bootup <- function(mylist){
foo <- bootstrap(mylist, 1000, mean)
return(quantile(foo$thetastar, 0.975)[1])
}
bootdown <- function(mylist){
foo <- bootstrap(mylist, 1000, mean)
return(quantile(foo$thetastar, 0.025)[1])
}
mybootup = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootup)
names(mybootup) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootup')
mybootdown = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootdown)
names(mybootdown) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootdown')
mystats = merge(mystats,mybootup)
mystats = merge(mystats,mybootdown)
forPower <- allSigChange %>%
filter(ROIMask == 'ToM', localizer = 'ToM', contrastName == 'joke-lit') %>%
filter(ROIName %in% c('RTPJ','LTPJ','PC','MM PFC') ) %>%
group_by(ROIName)%>%
summarise(m = mean(sigChange), sd = sd(sigChange), t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value)
rm(list = ls())
library(bootstrap)
library(dplyr)
library(ggplot2)
library(lme4)
library(pwr)
library(stringr)
library(tidyr)
setwd("/Users/mekline/Dropbox/_Projects/Jokes - fMRI/Jokes-Replication-Analysis/analysis_pipeline")
mywd = getwd()
#Make sure allSigChange is loaded. If it's not, load it
load("allSigChange.RData")
View(allSigChange)
9812/2
################ Exploratory analysis from Study 1: doing the power analysis for Study 2
#Let's try and do a power analysis on the Jokes results. (Considering a replication
#since a journal has asked for one) There are 4 regions we expect to
#find differences in: RTPJ, LTPJ, MMPFC, PC.  How big are those differences and how
#well powered are we?
cohens_d <- function(x, y) {
lx <- length(x)- 1
ly <- length(y)- 1
md  <- abs(mean(x) - mean(y))        ## mean difference (numerator)
csd <- lx * var(x) + ly * var(y)
csd <- csd/(lx + ly)
csd <- sqrt(csd)                     ## common sd computation
cd  <- md/csd                        ## cohen's d
}
forPower <- allSigChange %>%
filter(ROIMask == 'ToM', localizer = 'ToM', contrastName == 'joke-lit') %>%
filter(ROIName %in% c('RTPJ','LTPJ','PC','MM PFC') ) %>%
group_by(ROIName)%>%
summarise(m = mean(sigChange), sd = sd(sigChange), t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value)
forPower <- allSigChange %>%
filter(ROIMask == 'ToM', localizer == 'ToM', contrastName == 'joke-lit') %>%
filter(ROIName %in% c('RTPJ','LTPJ','PC','MM PFC') ) %>%
group_by(ROIName)%>%
summarise(m = mean(sigChange), sd = sd(sigChange), t = t.test(sigChange, mu=0,alt='greater')$statistic,
p = t.test(sigChange, mu=0,alt='greater')$p.value)
forPower
forPower$n <- 21
forPower$cohens_d <- forPower$m / forPower$sd
ptests <- mapply(pwr.t.test, n=forPower$n, d=forPower$cohens_d, sig.level=0.05, alternative='greater')
ptests
#This takes the individual-subject contrast values and runs some nifty lmer models.
#It assumes you've just run localizer_t_tests
#set wd
setwd("/Users/mekline/Dropbox/_Projects/Jokes - fMRI/Jokes-Replication-Analysis/analysis_pipeline")
#Make sure the data is loaded in!
#load("allSigChange.RData")
View(allSigChange)
#########
#EFFECT SIZE CALCULATION! Requested by the journal.  There is no standard way to report effect sizes for linear mixed
#models, so the approach we'll take is to report mean signal change values at the system level.  This is calculated
#over in the figure script (2figs_resp_jokes) since we generated those values there.
#########
# Get summary stats for the Jokes & JokesCustom tasks (used for graphing & reporting effect size measures)
#########
#Get Jokes values
JokeSigs = allSigChange %>%
filter(contrastName %in% c('joke','lit','high','med','low'))
#Next, get the table that we'll be making the graphs from: for each region (including the average region), take all
#the individual signal changes and calculate a mean and a standard error
sterr <- function(mylist){
my_se = sd(mylist)/sqrt(length(mylist))
return(my_se)
}
mystats = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), mean)
names(mystats) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'themean')
myster = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), sterr)
names(myster) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName', 'sterr')
mystats = merge(mystats,myster)
mystats$se_up = mystats$themean + mystats$sterr
mystats$se_down = mystats$themean - mystats$sterr
#Edit! We should be doing bootstrapped 95% confidence intervals instead! calculate them from allSigChange
#then merge into mystats
bootup <- function(mylist){
foo <- bootstrap(mylist, 1000, mean)
return(quantile(foo$thetastar, 0.975)[1])
}
bootdown <- function(mylist){
foo <- bootstrap(mylist, 1000, mean)
return(quantile(foo$thetastar, 0.025)[1])
}
mybootup = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootup)
names(mybootup) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootup')
mybootdown = aggregate(JokeSigs$sigChange, by=list(JokeSigs$ROIMask, JokeSigs$localizer, JokeSigs$task, JokeSigs$ROIName, JokeSigs$contrastName), bootdown)
names(mybootdown) = c('ROIMask','localizer', 'task', 'ROIName', 'contrastName',  'bootdown')
mystats = merge(mystats,mybootup)
mystats = merge(mystats,mybootdown)
eff <- mystats %>%
filter(ROIName == "LocalizerAverage") %>%
filter(contrastName == 'joke' | contrastName == 'lit') %>%
select(c("Group", "contrastName", "themean")) %>%
spread(contrastName, themean) %>%
mutate(sigChange = joke-lit)
eff <- mystats %>%
filter(ROIName == "LocalizerAverage") %>%
filter(contrastName == 'joke' | contrastName == 'lit') %>%
select(c("ROIMask", "localizer", "contrastName", "themean")) %>%
spread(contrastName, themean) %>%
mutate(sigChange = joke-lit)
eff
